
# ðŸ§  streamlit-local-llm

A minimal web-based UI built with [Streamlit](https://streamlit.io/) to run **local LLMs** using [llama.cpp](https://github.com/ggerganov/llama.cpp).  
Supports chat history, multiple models, and runs entirely offline with quantized GGUF models.

---

## ðŸš€ Features

- âœ… Local inference using `llama.cpp` backend
- âœ… Clean Streamlit UI with persistent chat history
- âœ… Works with any GGUF model (e.g. Q4_K_M, Q6_K)
- âœ… Sidebar with model selector and past threads
- âœ… No internet or OpenAI key required
- âœ… LAN access for other devices (e.g. Vision Pro)

---

## ðŸ“¦ Requirements

- Python 3.10+
- `llama-cpp-python` compiled with:
  - `metal` for Apple Silicon (M1/M2/M3)
  - `cuda` / `rocm` for NVIDIA / AMD
- GGUF-format quantized model (see [Hugging Face](https://huggingface.co/collections/ggml/gguf-models-656464cd5f7413fbb7b5fcb3))

### Install dependencies

```bash
pip install -r requirements.txtâ€“â€“
